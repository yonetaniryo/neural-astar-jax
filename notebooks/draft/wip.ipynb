{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9de3d0-4c15-486d-83fb-ce1ea52f9eee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from neural_astar_jax.planner import astar, differentiable_astar\n",
    "from neural_astar_jax.utils import data\n",
    "from neural_astar_jax.utils import training\n",
    "from importlib import reload\n",
    "reload(data)\n",
    "reload(training)\n",
    "reload(differentiable_astar)\n",
    "reload(astar)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "673c2751-8e5d-4970-b8c0-e2979f9566d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "from neural_astar.utils.data import create_dataloader\n",
    "\n",
    "train_loader_pt = create_dataloader(\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", \"train\", 100)\n",
    "val_loader_pt = create_dataloader(\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", \"valid\", 100, shuffle=False)\n",
    "\n",
    "from neural_astar.planner import NeuralAstar, VanillaAstar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "planner = NeuralAstar()\n",
    "va_planner = VanillaAstar()\n",
    "optimizer = optim.Adam(planner.parameters(), lr=0.001)\n",
    "val_batch = next(iter(val_loader_pt))\n",
    "step = 0\n",
    "for e in range(12):\n",
    "    for batch in train_loader_pt:\n",
    "        planner.train()\n",
    "        planner.astar.Tmax = 0.25\n",
    "        na_outputs = planner(batch[0], batch[1], batch[2])\n",
    "        # train_loss = nn.L1Loss()(na_outputs.histories, batch[3])\n",
    "        train_loss = torch.mean(torch.abs(na_outputs.histories - batch[3]))\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            planner.eval()\n",
    "            planner.astar.Tmax = 1.0\n",
    "            outputs = planner(val_batch[0], val_batch[1], val_batch[2])\n",
    "            val_loss = nn.L1Loss()(outputs.histories, val_batch[3])\n",
    "            va_outputs =va_planner(val_batch[0], val_batch[1], val_batch[2])\n",
    "\n",
    "            va_pathlen = va_outputs.paths.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "            na_pathlen = outputs.paths.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "            va_history = va_outputs.histories.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "            na_history = outputs.histories.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "            p_opt = (na_pathlen == va_pathlen).mean()\n",
    "            p_exp = np.maximum((va_history - na_history) / va_history, 0).mean()\n",
    "            h_mean = 2.0 / (1.0 / (p_opt + 1e-10) + 1.0 / (p_exp + 1e-10))\n",
    "\n",
    "            print(f\"{step=:02d}, {train_loss=:.4f}, {val_loss=:.4f}, {p_opt=:.4f}, {p_exp=:.4f}, {h_mean=:.4f}\")\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c63fe211-ca74-4540-b480-c3dfaf480de4",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "from neural_astar.planner import NeuralAstar, VanillaAstar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "train_loader = data.MazeDataLoader(filename=\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", split=\"train\", batch_size=100)\n",
    "val_loader = data.MazeDataLoader(filename=\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", split=\"val\", batch_size=1)\n",
    "\n",
    "\n",
    "planner = NeuralAstar()\n",
    "va_planner = VanillaAstar()\n",
    "optimizer = optim.Adam(planner.parameters(), lr=0.001)\n",
    "step = 0\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "val_batch = val_loader.load_all_instances(key)\n",
    "vmap_designs = torch.tensor(val_batch.map_design.to_py()).unsqueeze(1)\n",
    "vstart_maps = torch.tensor(val_batch.start_map.to_py()).unsqueeze(1)\n",
    "vgoal_maps = torch.tensor(val_batch.goal_map.to_py()).unsqueeze(1)\n",
    "vopt_trajs = torch.tensor(val_batch.path_map.to_py()).unsqueeze(1)\n",
    "for step in range(100):\n",
    "    key1, key = jax.random.split(key)\n",
    "    planner.train()\n",
    "    planner.astar.Tmax = 0.25\n",
    "    batch = train_loader.sample_batch(key1)\n",
    "    map_designs = torch.tensor(batch.map_design.to_py()).unsqueeze(1)\n",
    "    start_maps = torch.tensor(batch.start_map.to_py()).unsqueeze(1)\n",
    "    goal_maps = torch.tensor(batch.goal_map.to_py()).unsqueeze(1)\n",
    "    opt_trajs = torch.tensor(batch.path_map.to_py()).unsqueeze(1)\n",
    "    na_outputs = planner(map_designs, start_maps, goal_maps)\n",
    "    train_loss = nn.L1Loss()(na_outputs.histories, opt_trajs)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        planner.eval()\n",
    "        planner.astar.Tmax = 1.0\n",
    "        \n",
    "        outputs = planner(vmap_designs, vstart_maps, vgoal_maps)\n",
    "        val_loss = nn.L1Loss()(outputs.histories, vopt_trajs)\n",
    "        va_outputs = va_planner(vmap_designs, vstart_maps, vgoal_maps)\n",
    "\n",
    "        va_pathlen = va_outputs.paths.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "        na_pathlen = outputs.paths.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "        va_history = va_outputs.histories.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "        na_history = outputs.histories.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "        p_opt = (na_pathlen == va_pathlen).mean()\n",
    "        p_exp = np.maximum((va_history - na_history) / va_history, 0).mean()\n",
    "        h_mean = 2.0 / (1.0 / (p_opt + 1e-10) + 1.0 / (p_exp + 1e-10))\n",
    "\n",
    "        print(f\"{step=:02d}, {train_loss=:.4f}, {val_loss=:.4f}, {p_opt=:.4f}, {p_exp=:.4f}, {h_mean=:.4f}\")\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4d4b979-1047-4077-9e2b-968264844a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "planner = astar.NeuralAstar(is_training=True, search_step_ratio=0.25)\n",
    "train_loader = data.MazeDataLoader(filename=\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", split=\"train\", batch_size=100)\n",
    "val_loader = data.MazeDataLoader(filename=\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", split=\"val\", batch_size=1)\n",
    "trainer = training.Trainer(planner=planner, train_loader=train_loader, val_loader=val_loader, val_every_n_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea72b84c-17b6-4a74-9c56-b435bbc74912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=00, train_loss=0.0623, val_loss=0.1052, p_opt=0.5400, p_exp=0.3228, h_mean=0.4041\n",
      "best model updated (h_mean=0.4041 > best_h_mean=-inf)\n",
      "step=04, train_loss=0.0674, val_loss=0.0770, p_opt=0.4100, p_exp=0.4699, h_mean=0.4379\n",
      "best model updated (h_mean=0.4379 > best_h_mean=0.4041)\n",
      "step=08, train_loss=0.0681, val_loss=0.0817, p_opt=0.5200, p_exp=0.4380, h_mean=0.4755\n",
      "best model updated (h_mean=0.4755 > best_h_mean=0.4379)\n",
      "step=12, train_loss=0.0566, val_loss=0.0778, p_opt=0.5600, p_exp=0.4607, h_mean=0.5055\n",
      "best model updated (h_mean=0.5055 > best_h_mean=0.4755)\n",
      "step=16, train_loss=0.0576, val_loss=0.0761, p_opt=0.5900, p_exp=0.4699, h_mean=0.5231\n",
      "best model updated (h_mean=0.5231 > best_h_mean=0.5055)\n",
      "step=20, train_loss=0.0596, val_loss=0.0724, p_opt=0.4700, p_exp=0.4902, h_mean=0.4799\n",
      "step=24, train_loss=0.0620, val_loss=0.0743, p_opt=0.5200, p_exp=0.4831, h_mean=0.5009\n",
      "step=28, train_loss=0.0628, val_loss=0.0729, p_opt=0.5100, p_exp=0.4904, h_mean=0.5000\n",
      "step=32, train_loss=0.0626, val_loss=0.0735, p_opt=0.5600, p_exp=0.4816, h_mean=0.5179\n",
      "step=36, train_loss=0.0561, val_loss=0.0755, p_opt=0.5700, p_exp=0.4583, h_mean=0.5081\n",
      "step=40, train_loss=0.0617, val_loss=0.0713, p_opt=0.6000, p_exp=0.4911, h_mean=0.5401\n",
      "best model updated (h_mean=0.5401 > best_h_mean=0.5231)\n",
      "step=44, train_loss=0.0571, val_loss=0.0713, p_opt=0.5300, p_exp=0.4936, h_mean=0.5112\n",
      "step=48, train_loss=0.0670, val_loss=0.0727, p_opt=0.5500, p_exp=0.4840, h_mean=0.5149\n",
      "step=52, train_loss=0.0650, val_loss=0.0738, p_opt=0.5600, p_exp=0.4847, h_mean=0.5197\n",
      "step=56, train_loss=0.0547, val_loss=0.0721, p_opt=0.4900, p_exp=0.4955, h_mean=0.4927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = trainer.fit(jax.random.PRNGKey(0), max_steps=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b102bfb-53e1-4bf1-a00c-a11fbc412435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
