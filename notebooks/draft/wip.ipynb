{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f9de3d0-4c15-486d-83fb-ce1ea52f9eee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from neural_astar_jax.planner import astar, differentiable_astar\n",
    "from neural_astar_jax.utils import data\n",
    "from neural_astar_jax.utils import training\n",
    "from importlib import reload\n",
    "reload(data)\n",
    "reload(training)\n",
    "reload(differentiable_astar)\n",
    "reload(astar)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "673c2751-8e5d-4970-b8c0-e2979f9566d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "from neural_astar.utils.data import create_dataloader\n",
    "\n",
    "train_loader_pt = create_dataloader(\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", \"train\", 100)\n",
    "val_loader_pt = create_dataloader(\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", \"valid\", 100, shuffle=False)\n",
    "\n",
    "from neural_astar.planner import NeuralAstar, VanillaAstar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "planner = NeuralAstar()\n",
    "va_planner = VanillaAstar()\n",
    "optimizer = optim.Adam(planner.parameters(), lr=0.001)\n",
    "val_batch = next(iter(val_loader_pt))\n",
    "step = 0\n",
    "for e in range(12):\n",
    "    for batch in train_loader_pt:\n",
    "        planner.train()\n",
    "        planner.astar.Tmax = 0.25\n",
    "        na_outputs = planner(batch[0], batch[1], batch[2])\n",
    "        # train_loss = nn.L1Loss()(na_outputs.histories, batch[3])\n",
    "        train_loss = torch.mean(torch.abs(na_outputs.histories - batch[3]))\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            planner.eval()\n",
    "            planner.astar.Tmax = 1.0\n",
    "            outputs = planner(val_batch[0], val_batch[1], val_batch[2])\n",
    "            val_loss = nn.L1Loss()(outputs.histories, val_batch[3])\n",
    "            va_outputs =va_planner(val_batch[0], val_batch[1], val_batch[2])\n",
    "\n",
    "            va_pathlen = va_outputs.paths.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "            na_pathlen = outputs.paths.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "            va_history = va_outputs.histories.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "            na_history = outputs.histories.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "            p_opt = (na_pathlen == va_pathlen).mean()\n",
    "            p_exp = np.maximum((va_history - na_history) / va_history, 0).mean()\n",
    "            h_mean = 2.0 / (1.0 / (p_opt + 1e-10) + 1.0 / (p_exp + 1e-10))\n",
    "\n",
    "            print(f\"{step=:02d}, {train_loss=:.4f}, {val_loss=:.4f}, {p_opt=:.4f}, {p_exp=:.4f}, {h_mean=:.4f}\")\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c63fe211-ca74-4540-b480-c3dfaf480de4",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "from neural_astar.planner import NeuralAstar, VanillaAstar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "train_loader = data.MazeDataLoader(filename=\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", split=\"train\", batch_size=100)\n",
    "val_loader = data.MazeDataLoader(filename=\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", split=\"val\", batch_size=1)\n",
    "\n",
    "\n",
    "planner = NeuralAstar()\n",
    "va_planner = VanillaAstar()\n",
    "optimizer = optim.Adam(planner.parameters(), lr=0.001)\n",
    "step = 0\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "val_batch = val_loader.load_all_instances(key)\n",
    "vmap_designs = torch.tensor(val_batch.map_design.to_py()).unsqueeze(1)\n",
    "vstart_maps = torch.tensor(val_batch.start_map.to_py()).unsqueeze(1)\n",
    "vgoal_maps = torch.tensor(val_batch.goal_map.to_py()).unsqueeze(1)\n",
    "vopt_trajs = torch.tensor(val_batch.path_map.to_py()).unsqueeze(1)\n",
    "for step in range(100):\n",
    "    key1, key = jax.random.split(key)\n",
    "    planner.train()\n",
    "    planner.astar.Tmax = 0.25\n",
    "    batch = train_loader.sample_batch(key1)\n",
    "    map_designs = torch.tensor(batch.map_design.to_py()).unsqueeze(1)\n",
    "    start_maps = torch.tensor(batch.start_map.to_py()).unsqueeze(1)\n",
    "    goal_maps = torch.tensor(batch.goal_map.to_py()).unsqueeze(1)\n",
    "    opt_trajs = torch.tensor(batch.path_map.to_py()).unsqueeze(1)\n",
    "    na_outputs = planner(map_designs, start_maps, goal_maps)\n",
    "    train_loss = nn.L1Loss()(na_outputs.histories, opt_trajs)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        planner.eval()\n",
    "        planner.astar.Tmax = 1.0\n",
    "        \n",
    "        outputs = planner(vmap_designs, vstart_maps, vgoal_maps)\n",
    "        val_loss = nn.L1Loss()(outputs.histories, vopt_trajs)\n",
    "        va_outputs = va_planner(vmap_designs, vstart_maps, vgoal_maps)\n",
    "\n",
    "        va_pathlen = va_outputs.paths.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "        na_pathlen = outputs.paths.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "        va_history = va_outputs.histories.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "        na_history = outputs.histories.sum((1, 2, 3)).detach().cpu().numpy()\n",
    "        p_opt = (na_pathlen == va_pathlen).mean()\n",
    "        p_exp = np.maximum((va_history - na_history) / va_history, 0).mean()\n",
    "        h_mean = 2.0 / (1.0 / (p_opt + 1e-10) + 1.0 / (p_exp + 1e-10))\n",
    "\n",
    "        print(f\"{step=:02d}, {train_loss=:.4f}, {val_loss=:.4f}, {p_opt=:.4f}, {p_exp=:.4f}, {h_mean=:.4f}\")\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d4b979-1047-4077-9e2b-968264844a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "planner = astar.NeuralAstar(is_training=True, search_step_ratio=0.25)\n",
    "train_loader = data.MazeDataLoader(filename=\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", split=\"train\", batch_size=100)\n",
    "val_loader = data.MazeDataLoader(filename=\"../../planning-datasets/data/mpd/mazes_032_moore_c8.npz\", split=\"val\", batch_size=1)\n",
    "trainer = training.Trainer(planner=planner, train_loader=train_loader, val_loader=val_loader, val_every_n_steps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea72b84c-17b6-4a74-9c56-b435bbc74912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=00, train_loss=0.0623, val_loss=0.1103, p_opt=0.5800, p_exp=0.2934, h_mean=0.3897\n",
      "best model updated (h_mean=0.3897 > best_h_mean=-inf)\n",
      "step=04, train_loss=0.0674, val_loss=0.1126, p_opt=0.6400, p_exp=0.2861, h_mean=0.3954\n",
      "best model updated (h_mean=0.3954 > best_h_mean=0.3897)\n",
      "step=08, train_loss=0.0681, val_loss=0.1089, p_opt=0.5500, p_exp=0.3106, h_mean=0.3970\n",
      "best model updated (h_mean=0.3970 > best_h_mean=0.3954)\n",
      "step=12, train_loss=0.0566, val_loss=0.0966, p_opt=0.6000, p_exp=0.3762, h_mean=0.4625\n",
      "best model updated (h_mean=0.4625 > best_h_mean=0.3970)\n",
      "step=16, train_loss=0.0576, val_loss=0.0852, p_opt=0.5700, p_exp=0.4298, h_mean=0.4900\n",
      "best model updated (h_mean=0.4900 > best_h_mean=0.4625)\n",
      "step=20, train_loss=0.0596, val_loss=0.0771, p_opt=0.5300, p_exp=0.4744, h_mean=0.5007\n",
      "best model updated (h_mean=0.5007 > best_h_mean=0.4900)\n",
      "step=24, train_loss=0.0620, val_loss=0.0763, p_opt=0.5800, p_exp=0.4781, h_mean=0.5241\n",
      "best model updated (h_mean=0.5241 > best_h_mean=0.5007)\n",
      "step=28, train_loss=0.0628, val_loss=0.0718, p_opt=0.5300, p_exp=0.4987, h_mean=0.5139\n",
      "step=32, train_loss=0.0626, val_loss=0.0714, p_opt=0.5400, p_exp=0.4967, h_mean=0.5174\n",
      "step=36, train_loss=0.0561, val_loss=0.0734, p_opt=0.5600, p_exp=0.4749, h_mean=0.5139\n",
      "step=40, train_loss=0.0617, val_loss=0.0698, p_opt=0.5800, p_exp=0.5003, h_mean=0.5372\n",
      "best model updated (h_mean=0.5372 > best_h_mean=0.5241)\n",
      "step=44, train_loss=0.0571, val_loss=0.0692, p_opt=0.5200, p_exp=0.5024, h_mean=0.5110\n",
      "step=48, train_loss=0.0670, val_loss=0.0705, p_opt=0.5200, p_exp=0.4992, h_mean=0.5094\n",
      "step=52, train_loss=0.0650, val_loss=0.0714, p_opt=0.5200, p_exp=0.4958, h_mean=0.5076\n",
      "step=56, train_loss=0.0547, val_loss=0.0717, p_opt=0.5000, p_exp=0.4961, h_mean=0.4980\n",
      "step=60, train_loss=0.0580, val_loss=0.0739, p_opt=0.5900, p_exp=0.4841, h_mean=0.5318\n",
      "step=64, train_loss=0.0552, val_loss=0.0716, p_opt=0.5400, p_exp=0.4966, h_mean=0.5174\n",
      "step=68, train_loss=0.0567, val_loss=0.0693, p_opt=0.5700, p_exp=0.5022, h_mean=0.5339\n",
      "step=72, train_loss=0.0639, val_loss=0.0700, p_opt=0.4700, p_exp=0.5018, h_mean=0.4854\n",
      "step=76, train_loss=0.0530, val_loss=0.0711, p_opt=0.5300, p_exp=0.4995, h_mean=0.5143\n",
      "step=80, train_loss=0.0646, val_loss=0.0695, p_opt=0.5400, p_exp=0.5029, h_mean=0.5208\n",
      "step=84, train_loss=0.0628, val_loss=0.0747, p_opt=0.6000, p_exp=0.4848, h_mean=0.5363\n",
      "step=88, train_loss=0.0649, val_loss=0.0716, p_opt=0.5800, p_exp=0.4995, h_mean=0.5368\n",
      "step=92, train_loss=0.0580, val_loss=0.0687, p_opt=0.5900, p_exp=0.5098, h_mean=0.5470\n",
      "best model updated (h_mean=0.5470 > best_h_mean=0.5372)\n",
      "step=96, train_loss=0.0632, val_loss=0.0724, p_opt=0.5500, p_exp=0.4887, h_mean=0.5175\n",
      "step=100, train_loss=0.0644, val_loss=0.0706, p_opt=0.6000, p_exp=0.5040, h_mean=0.5479\n",
      "best model updated (h_mean=0.5479 > best_h_mean=0.5470)\n",
      "step=104, train_loss=0.0727, val_loss=0.0700, p_opt=0.6000, p_exp=0.5072, h_mean=0.5497\n",
      "best model updated (h_mean=0.5497 > best_h_mean=0.5479)\n",
      "step=108, train_loss=0.0608, val_loss=0.0725, p_opt=0.6100, p_exp=0.4969, h_mean=0.5477\n",
      "step=112, train_loss=0.0530, val_loss=0.0698, p_opt=0.6200, p_exp=0.5052, h_mean=0.5567\n",
      "best model updated (h_mean=0.5567 > best_h_mean=0.5497)\n",
      "step=116, train_loss=0.0558, val_loss=0.0730, p_opt=0.5700, p_exp=0.4993, h_mean=0.5323\n",
      "step=120, train_loss=0.0606, val_loss=0.0716, p_opt=0.5800, p_exp=0.5044, h_mean=0.5396\n",
      "step=124, train_loss=0.0544, val_loss=0.0721, p_opt=0.6200, p_exp=0.4983, h_mean=0.5525\n",
      "step=128, train_loss=0.0558, val_loss=0.0715, p_opt=0.5700, p_exp=0.4983, h_mean=0.5317\n",
      "step=132, train_loss=0.0672, val_loss=0.0715, p_opt=0.5600, p_exp=0.5019, h_mean=0.5294\n",
      "step=136, train_loss=0.0650, val_loss=0.0698, p_opt=0.6200, p_exp=0.5081, h_mean=0.5585\n",
      "best model updated (h_mean=0.5585 > best_h_mean=0.5567)\n",
      "step=140, train_loss=0.0601, val_loss=0.0726, p_opt=0.6200, p_exp=0.4995, h_mean=0.5532\n",
      "step=144, train_loss=0.0684, val_loss=0.0729, p_opt=0.5900, p_exp=0.4949, h_mean=0.5383\n",
      "step=148, train_loss=0.0574, val_loss=0.0719, p_opt=0.6400, p_exp=0.4977, h_mean=0.5600\n",
      "best model updated (h_mean=0.5600 > best_h_mean=0.5585)\n",
      "step=152, train_loss=0.0687, val_loss=0.0690, p_opt=0.5600, p_exp=0.5119, h_mean=0.5349\n",
      "step=156, train_loss=0.0598, val_loss=0.0701, p_opt=0.5800, p_exp=0.5055, h_mean=0.5402\n",
      "step=160, train_loss=0.0692, val_loss=0.0696, p_opt=0.6300, p_exp=0.5134, h_mean=0.5658\n",
      "best model updated (h_mean=0.5658 > best_h_mean=0.5600)\n",
      "step=164, train_loss=0.0558, val_loss=0.0696, p_opt=0.6100, p_exp=0.5075, h_mean=0.5541\n",
      "step=168, train_loss=0.0577, val_loss=0.0703, p_opt=0.6500, p_exp=0.5075, h_mean=0.5700\n",
      "best model updated (h_mean=0.5700 > best_h_mean=0.5658)\n",
      "step=172, train_loss=0.0600, val_loss=0.0688, p_opt=0.5800, p_exp=0.5117, h_mean=0.5437\n",
      "step=176, train_loss=0.0667, val_loss=0.0704, p_opt=0.5800, p_exp=0.5054, h_mean=0.5402\n",
      "step=180, train_loss=0.0498, val_loss=0.0713, p_opt=0.6200, p_exp=0.4947, h_mean=0.5503\n",
      "step=184, train_loss=0.0562, val_loss=0.0686, p_opt=0.5700, p_exp=0.5088, h_mean=0.5377\n",
      "step=188, train_loss=0.0643, val_loss=0.0699, p_opt=0.5600, p_exp=0.5074, h_mean=0.5324\n",
      "step=192, train_loss=0.0570, val_loss=0.0691, p_opt=0.6100, p_exp=0.5061, h_mean=0.5532\n",
      "step=196, train_loss=0.0567, val_loss=0.0695, p_opt=0.5900, p_exp=0.5038, h_mean=0.5435\n",
      "step=200, train_loss=0.0693, val_loss=0.0693, p_opt=0.5800, p_exp=0.5122, h_mean=0.5440\n",
      "step=204, train_loss=0.0638, val_loss=0.0689, p_opt=0.6000, p_exp=0.5163, h_mean=0.5550\n",
      "step=208, train_loss=0.0520, val_loss=0.0707, p_opt=0.6300, p_exp=0.5058, h_mean=0.5611\n",
      "step=212, train_loss=0.0559, val_loss=0.0697, p_opt=0.6200, p_exp=0.5094, h_mean=0.5593\n",
      "step=216, train_loss=0.0616, val_loss=0.0695, p_opt=0.6200, p_exp=0.5098, h_mean=0.5596\n",
      "step=220, train_loss=0.0649, val_loss=0.0680, p_opt=0.5600, p_exp=0.5182, h_mean=0.5383\n",
      "step=224, train_loss=0.0598, val_loss=0.0695, p_opt=0.6200, p_exp=0.5173, h_mean=0.5640\n",
      "step=228, train_loss=0.0630, val_loss=0.0714, p_opt=0.6200, p_exp=0.5004, h_mean=0.5538\n",
      "step=232, train_loss=0.0660, val_loss=0.0724, p_opt=0.6100, p_exp=0.4987, h_mean=0.5488\n",
      "step=236, train_loss=0.0625, val_loss=0.0697, p_opt=0.6300, p_exp=0.5114, h_mean=0.5645\n",
      "step=240, train_loss=0.0553, val_loss=0.0704, p_opt=0.5500, p_exp=0.5058, h_mean=0.5270\n",
      "step=244, train_loss=0.0620, val_loss=0.0705, p_opt=0.5900, p_exp=0.5024, h_mean=0.5427\n",
      "step=248, train_loss=0.0654, val_loss=0.0732, p_opt=0.5600, p_exp=0.4953, h_mean=0.5256\n",
      "step=252, train_loss=0.0745, val_loss=0.0707, p_opt=0.5600, p_exp=0.5079, h_mean=0.5327\n",
      "step=256, train_loss=0.0689, val_loss=0.0691, p_opt=0.5800, p_exp=0.5149, h_mean=0.5455\n",
      "step=260, train_loss=0.0645, val_loss=0.0724, p_opt=0.5800, p_exp=0.5037, h_mean=0.5391\n",
      "step=264, train_loss=0.0649, val_loss=0.0707, p_opt=0.5900, p_exp=0.5028, h_mean=0.5429\n",
      "step=268, train_loss=0.0586, val_loss=0.0692, p_opt=0.5800, p_exp=0.5088, h_mean=0.5420\n",
      "step=272, train_loss=0.0600, val_loss=0.0707, p_opt=0.5600, p_exp=0.5083, h_mean=0.5329\n",
      "step=276, train_loss=0.0648, val_loss=0.0695, p_opt=0.5500, p_exp=0.5122, h_mean=0.5304\n",
      "step=280, train_loss=0.0531, val_loss=0.0706, p_opt=0.5800, p_exp=0.5112, h_mean=0.5434\n",
      "step=284, train_loss=0.0618, val_loss=0.0709, p_opt=0.5800, p_exp=0.5102, h_mean=0.5429\n",
      "step=288, train_loss=0.0570, val_loss=0.0705, p_opt=0.6300, p_exp=0.5086, h_mean=0.5628\n",
      "step=292, train_loss=0.0581, val_loss=0.0696, p_opt=0.5200, p_exp=0.5092, h_mean=0.5146\n",
      "step=296, train_loss=0.0597, val_loss=0.0696, p_opt=0.6300, p_exp=0.5146, h_mean=0.5665\n",
      "step=300, train_loss=0.0573, val_loss=0.0685, p_opt=0.6000, p_exp=0.5155, h_mean=0.5546\n",
      "step=304, train_loss=0.0581, val_loss=0.0706, p_opt=0.6400, p_exp=0.5077, h_mean=0.5662\n",
      "step=308, train_loss=0.0685, val_loss=0.0712, p_opt=0.6000, p_exp=0.5036, h_mean=0.5476\n",
      "step=312, train_loss=0.0650, val_loss=0.0700, p_opt=0.6100, p_exp=0.5091, h_mean=0.5550\n",
      "step=316, train_loss=0.0575, val_loss=0.0702, p_opt=0.6100, p_exp=0.5097, h_mean=0.5553\n",
      "step=320, train_loss=0.0570, val_loss=0.0720, p_opt=0.5400, p_exp=0.4996, h_mean=0.5190\n",
      "step=324, train_loss=0.0556, val_loss=0.0699, p_opt=0.5400, p_exp=0.5073, h_mean=0.5232\n",
      "step=328, train_loss=0.0630, val_loss=0.0706, p_opt=0.6300, p_exp=0.5050, h_mean=0.5606\n",
      "step=332, train_loss=0.0618, val_loss=0.0698, p_opt=0.5700, p_exp=0.5093, h_mean=0.5379\n",
      "step=336, train_loss=0.0650, val_loss=0.0703, p_opt=0.6100, p_exp=0.5106, h_mean=0.5559\n",
      "step=340, train_loss=0.0506, val_loss=0.0714, p_opt=0.6000, p_exp=0.5084, h_mean=0.5504\n",
      "step=344, train_loss=0.0603, val_loss=0.0707, p_opt=0.6200, p_exp=0.5113, h_mean=0.5604\n",
      "step=348, train_loss=0.0580, val_loss=0.0706, p_opt=0.5700, p_exp=0.5106, h_mean=0.5387\n",
      "step=352, train_loss=0.0535, val_loss=0.0700, p_opt=0.5300, p_exp=0.5114, h_mean=0.5205\n",
      "step=356, train_loss=0.0503, val_loss=0.0693, p_opt=0.6000, p_exp=0.5148, h_mean=0.5541\n",
      "step=360, train_loss=0.0557, val_loss=0.0684, p_opt=0.5700, p_exp=0.5135, h_mean=0.5403\n",
      "step=364, train_loss=0.0578, val_loss=0.0706, p_opt=0.5400, p_exp=0.5059, h_mean=0.5224\n",
      "step=368, train_loss=0.0556, val_loss=0.0705, p_opt=0.5500, p_exp=0.5079, h_mean=0.5281\n",
      "step=372, train_loss=0.0576, val_loss=0.0688, p_opt=0.5200, p_exp=0.5141, h_mean=0.5170\n",
      "step=376, train_loss=0.0624, val_loss=0.0676, p_opt=0.6500, p_exp=0.5208, h_mean=0.5783\n",
      "best model updated (h_mean=0.5783 > best_h_mean=0.5700)\n",
      "step=380, train_loss=0.0518, val_loss=0.0698, p_opt=0.6100, p_exp=0.5125, h_mean=0.5570\n",
      "step=384, train_loss=0.0713, val_loss=0.0710, p_opt=0.6200, p_exp=0.5048, h_mean=0.5565\n",
      "step=388, train_loss=0.0685, val_loss=0.0705, p_opt=0.6200, p_exp=0.5112, h_mean=0.5604\n",
      "step=392, train_loss=0.0606, val_loss=0.0700, p_opt=0.6000, p_exp=0.5127, h_mean=0.5529\n",
      "step=396, train_loss=0.0670, val_loss=0.0699, p_opt=0.5800, p_exp=0.5142, h_mean=0.5451\n"
     ]
    }
   ],
   "source": [
    "state = trainer.fit(jax.random.PRNGKey(0), max_steps=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b102bfb-53e1-4bf1-a00c-a11fbc412435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
